{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\python311\\lib\\site-packages (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_KEY= os.environ.get(\"OPENIA_KEY2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic assitant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[Assistant](data=[Assistant(id='asst_FX02iTGdLRm6GJqEsTcBqwMi', created_at=1709622658, description=None, file_ids=[], instructions=None, metadata={}, model='gpt-4-1106-preview', name=None, object='assistant', tools=[]), Assistant(id='asst_yIG7ie5crK3fN1a4D8ytFGiH', created_at=1704583711, description=None, file_ids=[], instructions='You are a personal data analytics specialized in ghapics. When a  user enter data , write and run code to generate charts.', metadata={}, model='gpt-4-1106-preview', name='Data Analytics with retrival', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter'), ToolRetrieval(type='retrieval')]), Assistant(id='asst_FXFcfrXSThWojfsAtw1mAH2e', created_at=1704483770, description=None, file_ids=[], instructions='You are a personal math tutor. Write and run code to answer math questions.', metadata={}, model='gpt-4-1106-preview', name='Math Tutor', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]), Assistant(id='asst_9nKBRu2bGHN9gZ4aH5WoaQ1Z', created_at=1704443529, description=None, file_ids=[], instructions='You are a personal data analytics specialized in ghapics. When a  user enter data , write and run code to generate a char.', metadata={}, model='gpt-4', name='Data Analytics', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])], object='list', first_id='asst_FX02iTGdLRm6GJqEsTcBqwMi', last_id='asst_9nKBRu2bGHN9gZ4aH5WoaQ1Z', has_more=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get list of assistants\n",
    "assistants = client.beta.assistants.list()\n",
    "assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=\"gpt-4-1106-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a thread for the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thread(id='thread_tQJtoO248QjDe6RksVG1pExf', created_at=1704483865, metadata={}, object='thread')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. add a message to the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreadMessage(id='msg_UjBnz8OqZAvMIJAdh7bECBhf', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='I need to solve the equation `3x + 11 = 14`. Can you help me?'), type='text')], created_at=1704484444, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_tQJtoO248QjDe6RksVG1pExf')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. run the assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Write code to solve the equation `3x + 11 = 14`.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. check the run status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(id='run_tC6B9j3ziccvdApzVx0Jw9Jp', assistant_id='asst_FXFcfrXSThWojfsAtw1mAH2e', cancelled_at=None, completed_at=1704484542, created_at=1704484532, expires_at=None, failed_at=None, file_ids=[], instructions='Write code to solve the equation `3x + 11 = 14`.', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=1704484533, status='completed', thread_id='thread_tQJtoO248QjDe6RksVG1pExf', tools=[ToolAssistantToolsCode(type='code_interpreter')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Display Assistant's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_e8ADPhYCXGL9HuAl6aJ7a61g', assistant_id='asst_FXFcfrXSThWojfsAtw1mAH2e', content=[MessageContentText(text=Text(annotations=[], value='The solution to the equation \\\\(3x + 11 = 14\\\\) is \\\\(x = 1\\\\).'), type='text')], created_at=1704484542, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_tC6B9j3ziccvdApzVx0Jw9Jp', thread_id='thread_tQJtoO248QjDe6RksVG1pExf'), ThreadMessage(id='msg_UjBnz8OqZAvMIJAdh7bECBhf', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='I need to solve the equation `3x + 11 = 14`. Can you help me?'), type='text')], created_at=1704484444, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_tQJtoO248QjDe6RksVG1pExf')], object='list', first_id='msg_e8ADPhYCXGL9HuAl6aJ7a61g', last_id='msg_UjBnz8OqZAvMIJAdh7bECBhf', has_more=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Data Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant(id='asst_FXFcfrXSThWojfsAtw1mAH2e', created_at=1704483770, description=None, file_ids=[], instructions='You are a personal math tutor. Write and run code to answer math questions.', metadata={}, model='gpt-4-1106-preview', name='Math Tutor', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrive the assitant we create in playground\n",
    "assistant = client.beta.assistants.retrieve(assistant_id=\"asst_FXFcfrXSThWojfsAtw1mAH2e\")\n",
    "assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thread(id='thread_Id0XhhagHm53eRM4RYcU4F0C', created_at=1704485982, metadata={}, object='thread')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the thread\n",
    "thread = client.beta.threads.create()\n",
    "thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create a chart with this information ```cities = [\"Tokyo\", \"Delhi\", \"Shanghai\", \"Sao Paulo\", \"Mumbai\"]\n",
      "populations = [37.4, 31.0, 27.1, 22.2, 20.7]```\n"
     ]
    }
   ],
   "source": [
    "# Open the text file in read mode\n",
    "with open('./prompts/DataAnalysisClientInstruction.txt', 'r') as file:\n",
    "    # Read the entire file content into a string\n",
    "    prompt_content = file.read()\n",
    "print(prompt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the assistant\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=prompt_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-ly9Chl6UFDPPXODgRnFXBYgG\n"
     ]
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "file_id=messages.data[0].content[0].image_file.file_id\n",
    "print(file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = client.files.content(file_id)\n",
    "image_data_bytes = image_data.read()\n",
    "\n",
    "with open(\"./filesContent/my-image.png\", \"wb\") as file:\n",
    "    file.write(image_data_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant with retrivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the assistant\n",
    "assistant = client.beta.assistants.retrieve(\"asst_yIG7ie5crK3fN1a4D8ytFGiH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './files/MikeTheMason-GMC-October-2022.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Upload file to OpenAI\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m----> 3\u001b[0m   file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./files/MikeTheMason-GMC-October-2022.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      4\u001b[0m   purpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistants\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m file\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './files/MikeTheMason-GMC-October-2022.json'"
     ]
    }
   ],
   "source": [
    "# #Upload file to OpenAI\n",
    "# file = client.files.create(\n",
    "#   file=open(\"./files/MikeTheMason-GMC-October-2022.json\", \"rb\"),\n",
    "#   purpose='assistants'\n",
    "# )\n",
    "\n",
    "# file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add the file in two ways, adding the file to the assistant or to the message youre about to send to threat to then be used by the assistant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the file to the message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Read this json file, and create 3 charts. impressions over time, click over time and conversions over time\",\n",
    "    file_ids=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Run(id='run_DTFYvb0wdEA5WaXGzDn3Eo73', assistant_id='asst_yIG7ie5crK3fN1a4D8ytFGiH', cancelled_at=None, completed_at=None, created_at=1709706237, expires_at=1709706837, failed_at=None, file_ids=[], instructions='Please respond with just the charts created from the json file. without any other text.', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_QkpLO82I8Eqo29FqohbhfS9L', tools=[ToolAssistantToolsCode(type='code_interpreter'), ToolAssistantToolsRetrieval(type='retrieval')], usage=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    "  instructions=\"Please respond with just the charts created from the json file. without any other text.\"\n",
    ")\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.retrieve(\n",
    "  thread_id=thread.id,\n",
    "  run_id=run.id\n",
    ")\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thread_QkpLO82I8Eqo29FqohbhfS9L'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_message = messages.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears there is an issue with accessing the file. Please make sure you have uploaded the JSON file you are referring to, so I can create the charts for impressions over time, clicks over time, and conversions over time.\n"
     ]
    }
   ],
   "source": [
    "for contentObject in last_message.content:\n",
    "    if contentObject.type == \"image_file\":\n",
    "        print('test')\n",
    "    elif contentObject.type == \"text\":\n",
    "        text = contentObject.text.value\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id=\"file-zwGABDv4yKaKX6zoCN4vUhiJ\"\n",
    "image_data = client.files.content(file_id)\n",
    "image_data_bytes = image_data.read()\n",
    "\n",
    "with open(\"./files/retrival.png\", \"wb\") as file:\n",
    "    file.write(image_data_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "event={\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"Sns\": {\n",
    "        \"Message\": {\n",
    "          \"file_id_list\": [\n",
    "            \"file-j4VvaQ2IV0Turw10Tm6zoSEv\"\n",
    "          ],\n",
    "          \"api_key\": \"key\",\n",
    "          \"assistant_id\":\"asst_yIG7ie5crK3fN1a4D8ytFGiH\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileId ['file-j4VvaQ2IV0Turw10Tm6zoSEv']\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "sns_message=event['Records'][0]['Sns']['Message']\n",
    "\n",
    "print('fileId',sns_message['file_id_list'])\n",
    "client = OpenAI(api_key=sns_message['api_key'])\n",
    "assistant = client.beta.assistants.retrieve(sns_message['assistant_id'])\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "#Add the file to threat as a message\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Read this json file, and create 3 charts. impressions over time, click over time and conversions over time\",\n",
    "    file_ids=sns_message['file_id_list']\n",
    ")\n",
    "\n",
    "#makes OpenAI assistant to run and generate the response\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"Please respond with just the charts created from the json file. without any other text.\"\n",
    ")\n",
    "\n",
    "status = run.status\n",
    "while status == \"in_progress\" or status == \"queued\":\n",
    "    time.sleep(1)\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    status = run.status\n",
    "    print(status)\n",
    "\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "last_message = messages.data[0]\n",
    "\n",
    "last_message_files=client.beta.threads.messages.files.list(\n",
    "    thread_id=thread.id,\n",
    "    message_id=last_message.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_iHEnNONZn0jPaSsawbuH7dHa', assistant_id='asst_yIG7ie5crK3fN1a4D8ytFGiH', cancelled_at=None, completed_at=1706049301, created_at=1706049224, expires_at=None, failed_at=None, file_ids=[], instructions='Please respond with just the charts created from the json file. without any other text.', last_error=None, metadata={}, model='gpt-4-1106-preview', object='thread.run', required_action=None, started_at=1706049225, status='completed', thread_id='thread_j6WMjrFWmXebHaG6kUa5zqUk', tools=[ToolAssistantToolsCode(type='code_interpreter'), ToolAssistantToolsRetrieval(type='retrieval')], usage={'prompt_tokens': 6471, 'completion_tokens': 712, 'total_tokens': 7183})\n"
     ]
    }
   ],
   "source": [
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MessageFile(id='file-j4VvaQ2IV0Turw10Tm6zoSEv', created_at=1706198238, message_id='msg_vuwbeSfxBuNA6lfpFlFAhMWQ', object='thread.message.file')]\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Not allowed to download files of purpose: assistants', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m last_message_files\u001b[38;5;241m=\u001b[39mclient\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mthreads\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m      3\u001b[0m     thread_id\u001b[38;5;241m=\u001b[39mthread\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m      4\u001b[0m     message_id\u001b[38;5;241m=\u001b[39mlast_message\u001b[38;5;241m.\u001b[39mid\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_message_files\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m----> 8\u001b[0m image_data \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_message_files\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\resources\\files.py:229\u001b[0m, in \u001b[0;36mFiles.content\u001b[1;34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontent\u001b[39m(\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    208\u001b[0m     file_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m HttpxBinaryResponseContent:\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Returns the contents of the specified file.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/files/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHttpxBinaryResponseContent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\_base_client.py:1031\u001b[0m, in \u001b[0;36mSyncAPIClient.get\u001b[1;34m(self, path, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1028\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;66;03m# cast is required because mypy complains about returning Any even though\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;66;03m# it understands the type variables\u001b[39;00m\n\u001b[1;32m-> 1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Not allowed to download files of purpose: assistants', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "last_message = messages.data[-1]\n",
    "last_message_files=client.beta.threads.messages.files.list(\n",
    "    thread_id=\"thread_ZUzBh1MJ7iVplH8WtMsFOUIB\",\n",
    "    message_id=\"msg_GOZ3PwgzUYiZmipVmNKKB1sB\"\n",
    ")\n",
    "print(last_message_files.data)\n",
    "\n",
    "image_data = client.files.content(last_message_files.data[0].id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
