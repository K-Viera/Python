{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n\\n1. Justin Bieber was born on March 1, 1994.\\n2. The Super Bowl is played in February of each year.\\n3. Therefore, the Super Bowl for the year of Justin Bieber's birth would have been Super Bowl XXVIII, played on January 30, 1994.\\n4. The Dallas Cowboys defeated the Buffalo Bills in Super Bowl XXVIII with a score of 30-13.\\n5. Therefore, the Dallas Cowboys won the Super Bowl in the year Justin Bieber was born.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'adore programmer.\" response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 34, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}\n",
      "Tokens Used: 40\n",
      "\tPrompt Tokens: 34\n",
      "\tCompletion Tokens: 6\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $6.300000000000001e-05\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0)\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that translates English to French.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. I love programming.\"\n",
    "    ),\n",
    "]\n",
    "with get_openai_callback() as cb:\n",
    "    print(chat.invoke(messages))\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['target_language', 'text'], template='\\nyour task is to translate the following text to {target_language}:\\n\\n\"{text}\"\\n')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "your task is to translate the following text to {target_language}:\n",
    "\n",
    "\"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"Me encanta programar mucho en mi tiempo libre\"', 'target_language': 'spanish'}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.7,model_name=\"gpt-3.5-turbo\")\n",
    "chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "text = \"I love programming a lot in my free time\"\n",
    "response = chain.invoke(input={\"text\": text, \"target_language\": \"spanish\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7,model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"apple\",\n",
      "        \"amount\": 10\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"orange\",\n",
      "        \"amount\": 5\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"banana\",\n",
      "        \"amount\": 8\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"apple\",\n",
      "        \"amount\": 15\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "template1 = \"\"\"\n",
    "Create an {amount} of items in a JSON list following the schema below, try to repeat at least 1 product name more than once.:\n",
    "{{\n",
    "    name: str,\n",
    "    amount: int,\n",
    "}}\n",
    "\"\"\"\n",
    "prompt_template1 = PromptTemplate.from_template(template1)\n",
    "chain1 = LLMChain(prompt=prompt_template1, llm=llm)\n",
    "response = chain1.invoke(\"4\")\n",
    "print(response[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\"from the JSON list {data}, get the mean, median, and mode of the data return it in a JSON object.\"\"\"\n",
    "prompt_template2 = PromptTemplate.from_template(template2)\n",
    "chain2 = LLMChain(prompt=prompt_template2, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m[\n",
      "    {\n",
      "        \"name\": \"Apple\",\n",
      "        \"amount\": 5\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Banana\",\n",
      "        \"amount\": 3\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Orange\",\n",
      "        \"amount\": 2\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Grapes\",\n",
      "        \"amount\": 4\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Apple\",\n",
      "        \"amount\": 2\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"Pineapple\",\n",
      "        \"amount\": 1\n",
      "    }\n",
      "]\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\n",
      "    \"mean\": 3.17,\n",
      "    \"median\": 3,\n",
      "    \"mode\": \"Apple\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2],verbose=True)\n",
    "output = overall_chain.invoke(\"6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"mean\": 3.17,\n",
      "    \"median\": 3,\n",
      "    \"mode\": \"Apple\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Agent (RAG)\n",
    "allow your model to interact with real world data or external services, like databases or web APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[2, 4, 6, 8]\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range (1, 10) if n % 2 == 0])')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.7 , model_name=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_python_agent(llm=llm, tool=PythonREPLTool(),verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo find the mean of the first 10 even numbers, I'll first generate the list of these numbers, then calculate their mean, and finally format the result to display 3 decimals.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: even_numbers = list(range(2, 21, 2))\n",
      "print(even_numbers)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have generated the list of the first 10 even numbers successfully. Next, I'll calculate the mean of these numbers and format the result to display 3 decimals.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: mean_even_numbers = sum(even_numbers) / len(even_numbers)\n",
      "print(f\"{mean_even_numbers:.3f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m11.000\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: The mean of the first 10 even numbers, formatted to display 3 decimals, is 11.000.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Find the mean of the first 10 even numbers and print the result, force to display 3 decimals.'\n",
    "response = agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The mean of the first 10 even numbers, formatted to display 3 decimals, is 11.000.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
